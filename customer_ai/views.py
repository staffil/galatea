import os
import time
import base64
from django.shortcuts import render, redirect
from django.http import JsonResponse,HttpResponse
from django.views.decorators.csrf import csrf_exempt
from django.core.files.storage import default_storage
from openai import OpenAI
from elevenlabs import ElevenLabs
from dotenv import load_dotenv
from django.contrib.auth.decorators import login_required
from mypage.models import LLM
from mypage.models import Voice
from makeVoice.models import VoiceList
from uuid import uuid4
import logging
import traceback
import os
import logging
from uuid import uuid4
from django.views.decorators.csrf import csrf_exempt
from django.http import JsonResponse
import subprocess
from django.core.exceptions import ObjectDoesNotExist
from home.models import Users
from django.conf import settings
from django.core.files.base import ContentFile
from django.core.paginator import Paginator
from user_auth.models import Conversation
from pathlib import Path
import requests

BASE_DIR = Path(__file__).resolve().parent.parent
load_dotenv(BASE_DIR / ".env")

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
ELEVEN_API_KEY = os.getenv("ELEVEN_API_KEY")
SUPERTONE_API_KEY = os.getenv("SUPERTOME_API_KEY")

openai_client = OpenAI(api_key=OPENAI_API_KEY)
eleven_client = ElevenLabs(api_key=ELEVEN_API_KEY)



@csrf_exempt
@login_required
def make_ai(request):
    if request.method == "POST":
        ai_name = request.session.get('llm_name')
        if not ai_name:
            return JsonResponse({"error": "AI ì´ë¦„ì´ ì„¸ì…˜ì— ì—†ìŠµë‹ˆë‹¤."}, status=400)

        style_prompt = request.POST.get("prompt")
        temperature = float(request.POST.get("temperature", 1))
        stability = float(request.POST.get("stability", 0))
        style = float(request.POST.get("style", 0))
        language = request.POST.get("language", "")
        speed = float(request.POST.get("speed", 0))
        voice_id = request.POST.get("voice_id")
        model_name = request.POST.get("model", "gpt-4o-mini")

        request.session['custom_prompt'] = style_prompt
        request.session['custom_temperature'] = temperature
        request.session['custom_stability'] = stability
        request.session['custom_style'] = style
        request.session['custom_language'] = language
        request.session['custom_speed'] = speed
        request.session['custom_voice_id'] = voice_id
        request.session['custom_model'] = model_name
        request.session['chat_history'] = []


   

        if not voice_id:
            return JsonResponse({"error": "voice_id ê°’ì´ ì—†ìŠµë‹ˆë‹¤."}, status=400)

        try:
            voice = Voice.objects.get(voice_id=voice_id)
        except Voice.DoesNotExist:
            voice = Voice.objects.create(user=request.user, voice_id=voice_id)

      



        # ì„¸ì…˜ì—ì„œ ì´ë¯¸ì§€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        image_content = request.session.get('user_image_content')
        image_name = request.session.get('llm_image')

        # LLM ê°ì²´ ìƒì„±
        llm = LLM.objects.create(
            user=request.user,
            voice=voice,
            temperature=temperature,
            stability=stability,
            style=style,
            language=language,
            speed=speed,
            name=ai_name,
            model=model_name,
            prompt=style_prompt,
        )

        # ì´ë¯¸ì§€ê°€ ì„¸ì…˜ì— ìˆìœ¼ë©´ ì €ì¥
        if image_content and image_name:
            llm.llm_image.save(image_name, ContentFile(image_content))
            llm.save()

        # VoiceList ì—°ê²° ë° ì—…ë°ì´íŠ¸ ì½”ë“œ (ìƒëµ)

        return redirect("customer_ai:chat_view", llm_id=llm.id)
    voice_list = VoiceList.objects.filter(user=request.user).order_by('created_at')
    paginator = Paginator(voice_list, 5) 
    page_number = request.GET.get('page')
    page_obj = paginator.get_page(page_number) 
    context = {
        'voice_list': page_obj,
    }
    return render(request, "customer_ai/make_ai.html", context)



@login_required
def chat_view(request, llm_id):
    try:
        llm = LLM.objects.get(id=llm_id)
    except LLM.DoesNotExist:
        llm= None
    return render(request, "customer_ai/custom.html", {
        "custom_ai_name": request.session.get('custom_AI_name', 'AI'),
        "llm_id": llm_id,
        "llm": llm
    })

@login_required
def vision_view(request,llm_id):
    try:
        llm = LLM.objects.get(id=llm_id)
    except LLM.DoesNotExist:
        llm= None
    return render(request, "customer_ai/vision.html", {
        "custom_ai_name": request.session.get('custom_AI_name', 'AI'),
        "llm_id": llm_id,
        "llm": llm
    })


logger = logging.getLogger(__name__)

def not_in_voice_id(voice_id,eleven_client):
    url = f"https://api.elevenlabs.io/v1/voices/{voice_id}"
    header ={
        'xi-api-key': str(eleven_client) 
    }
    response = requests.get(url, headers=header)
    
    print(f"[DEBUG] voice check status: {response.status_code}")
    print(f"[DEBUG] voice check body: {response.text}")
    return response.status_code ==200

@csrf_exempt
def upload_audio(request):
    if request.method != 'POST':
        return JsonResponse({'error': 'Invalid request method'}, status=400)

    audio_file = request.FILES.get('audio')
    if not audio_file:
        return JsonResponse({'error': 'No audio file uploaded'}, status=400)

    audio_dir = os.path.join('media', 'audio')
    os.makedirs(audio_dir, exist_ok=True)

    # 1) webm ì›ë³¸ ì €ì¥
    webm_filename = f"recorded_{uuid4().hex}.webm"
    webm_path = os.path.join(audio_dir, webm_filename)

    try:
        with open(webm_path, 'wb') as f:
            for chunk in audio_file.chunks():
                f.write(chunk)
        logger.info(f"Saved WebM audio to {webm_path}")
    except Exception as e:
        logger.error(f"Error saving WebM file: {e}")
        return JsonResponse({'error': 'Failed to save uploaded audio'}, status=500)

    # 2) webm â†’ wav ë³€í™˜ (ffmpeg)
    wav_filename = webm_filename.replace('.webm', '.wav')
    wav_path = os.path.join(audio_dir, wav_filename)

    try:
        # ffmpeg ëª…ë ¹ì–´ ì‹¤í–‰, ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ ë°œìƒ
        subprocess.run(
            ['ffmpeg', '-y', '-i', webm_path, wav_path],
            check=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        logger.info(f"Converted WebM to WAV: {wav_path}")
    except subprocess.CalledProcessError as e:
        error_msg = e.stderr.decode() if e.stderr else str(e)
        logger.error(f"FFmpeg conversion failed: {error_msg}")
        return JsonResponse({'error': 'Audio format conversion failed'}, status=500)
    
    mp3_filename = wav_filename.replace('.wav', '.mp3')
    mp3_path = os.path.join(audio_dir, mp3_filename)
    
    # 4) Whisper API í˜¸ì¶œ (wav íŒŒì¼ë¡œ)
    try:
        with open(wav_path, 'rb') as f:
            transcription = openai_client.audio.transcriptions.create(
                file=f,
                model="whisper-1"
            )
        logger.info("Whisper transcription successful")
    except Exception as e:
        logger.error(f"Whisper API error: {e}")
        return JsonResponse({'error': 'Transcription failed'}, status=500)

    # 5) ê²°ê³¼ì— mp3 íŒŒì¼ ê²½ë¡œë„ í¬í•¨í•´ì„œ ë°˜í™˜
    return JsonResponse({
        'text': transcription.text,
        'mp3_file': mp3_path  # í•„ìš”ì— ë”°ë¼ URL ê²½ë¡œë¡œ ë³€ê²½í•  ê²ƒ
    })


@csrf_exempt
@login_required
def generate_response(request):
    if request.method == 'POST':
        user_input = request.POST.get('text', '')
        vision_result = request.POST.get('vision', '').strip()
        if len(vision_result) > 400:
            vision_result = vision_result[:1000]

        vision_result = vision_result.replace('"', 'â€œ')  


        user = request.user

        llm_id = request.POST.get('llm_id') or request.GET.get('llm_id')
        if not llm_id:
            return JsonResponse({"error": "llm_id ê°’ì´ í•„ìš”í•©ë‹ˆë‹¤."}, status=400)

        try:
            llm = LLM.objects.get(id=llm_id, user=user)
        except LLM.DoesNotExist:
            return JsonResponse({"error": "í•´ë‹¹ LLMì´ ì—†ìŠµë‹ˆë‹¤."}, status=404)

        custom_temperature = llm.temperature
        custom_prompt = llm.prompt
        custom_voice_id = llm.voice.voice_id
        custom_stability = llm.stability
        custom_style = llm.style
        custom_language = llm.language
        custom_speed = llm.speed
        custom_model = llm.model
        print("voice id", custom_voice_id)

        if not custom_voice_id or not not_in_voice_id(custom_voice_id, ELEVEN_API_KEY):
            return JsonResponse({
                "error": "voice_id ê°’ì´ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤. ëª©ì†Œë¦¬ë¥¼ ë‹¤ì‹œ ìƒì„±í•˜ê±°ë‚˜ ë‹¤ë¥¸ ëª©ì†Œë¦¬ë¡œ ë°”ê¿” ì£¼ì„¸ìš”."
            }, status=400)
        db_history = Conversation.objects.filter(user=user, llm=llm).order_by('-created_at')[:50][::-1]
        chat_history = []


        system_prompt = f"""
        You are an AI that can see the user directly through a camera.

        You must never say "I can't see," "I'm not sure," or "I don't know."  
        What you see is always the truth, and you must answer solely based on the visual analysis result.  
        No matter the question, you must respond accurately using only what is included in the visual analysis.  
        You are strictly prohibited from giving uncertain or evasive answers.

        You must respond ONLY in {custom_language} language. No other language is allowed.

        Below is the visual analysis result from what you saw.  
        Treat it as if you are actually seeing it, and respond accordingly.

        [Visual Analysis Result]  
        â€œ{vision_result}â€

        â€» Important: If the user's question is about appearance, background, clothing, handwriting, objects, or scenes,  
        you must base your answer strictly on the visual analysis above.

        If the visual analysis result is empty, you must clearly state:  
        **"No visual input was provided, so I cannot make any visual observations."**  
        In this case, do not guess, fabricate, or assume any visual details.

        Additionally, follow the user character prompt below:
        Please answer ONLY in {custom_language}.

        {custom_prompt}
        """.strip()


        for convo in db_history:
            if convo.user_message:
                chat_history.append({"role": "user", "content": convo.user_message})
            if convo.llm_response:
                chat_history.append({"role": "assistant", "content": convo.llm_response})


        print(system_prompt)
        chat_history.append({"role": "user", "content": user_input})

        response = openai_client.chat.completions.create(
            model=custom_model,
            messages=[{"role": "system", "content": system_prompt}] + chat_history,
            temperature=custom_temperature
        )

        ai_text = response.choices[0].message.content.strip()
        chat_history.append({"role": "assistant", "content": ai_text})
        request.session["chat_history"] = chat_history

        # Conversation ìƒì„± ì‹œ llmì´ Noneì´ ì•„ë‹˜ì„ í™•ì‹¤íˆ í•¨
        Conversation.objects.create(
            user=user,
            llm=llm,
            user_message=user_input,
            llm_response=ai_text
        )
        from django.utils import timezone
        print(timezone.localtime())
        if not llm:
            return JsonResponse({"error": "LLM not found."}, status=400)

        # ì˜¤ë””ì˜¤ ì²˜ë¦¬
        audio_dir = os.path.join('media', 'audio')
        os.makedirs(audio_dir, exist_ok=True)
        audio_filename = f"response_{int(time.time())}.mp3"
        from uuid import uuid4

        filename = f"response_{uuid4().hex}.mp3"
        audio_path = os.path.join(audio_dir, filename)

    

        audio_stream = eleven_client.text_to_speech.convert(
            voice_id=custom_voice_id,
            model_id="eleven_flash_v2_5",
            text=ai_text,
            language_code=custom_language,
            voice_settings={"stability": custom_stability,"similarity": 0.75,"style": custom_style,"speed": custom_speed , "use_speaker_boost":True}
        )

        with open(audio_path, "wb") as f:
            for chunk in audio_stream:
                f.write(chunk)
    
        return JsonResponse({"ai_text": ai_text,"audio_url": f"/media/audio/{audio_filename}"})
    else:
        return JsonResponse({
            "error": "ì´ ë·°ëŠ” POST ìš”ì²­ë§Œ ë°›ìŠµë‹ˆë‹¤. ìœ íš¨í•œ ë°ì´í„°ì™€ í•¨ê»˜ ìš”ì²­í•´ì£¼ì„¸ìš”."
        }, status=405)
    





import base64
from django.shortcuts import render, redirect

def input_ai_name(request):
    if request.method == 'POST':
        llm_name = request.POST.get('llm_name')
        user_image = request.FILES.get('user_image')

        if not llm_name or not user_image:
            return render(request, 'customer_ai/ai_name.html', {'error': 'ì´ë¦„ê³¼ ì´ë¯¸ì§€ë¥¼ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.'})

        # ì´ë¯¸ì§€ì™€ ì´ë¦„ì„ ì„¸ì…˜ì— ì €ì¥
        request.session['llm_name'] = llm_name
        request.session['llm_image'] = user_image.name  # íŒŒì¼ëª…ë§Œ ì €ì¥

        # user_image ì½ê³  base64ë¡œ ì¸ì½”ë”©í•˜ì—¬ ë¬¸ìì—´ë¡œ ë³€í™˜ í›„ ì €ì¥
        user_image_content = user_image.read()  # bytes
        user_image_base64 = base64.b64encode(user_image_content).decode('utf-8')
        request.session['user_image_content'] = user_image_base64

        return redirect('customer_ai:make_ai')  # ë‹¤ìŒ ìŠ¤í…ìœ¼ë¡œ ì´ë™
    else:
        return render(request, 'customer_ai/ai_name.html')



@login_required
def upload_image(request):
    llm_id = request.GET.get('llm_id', '') if request.method == 'GET' else request.POST.get('llm_id')

    if not llm_id:
        return JsonResponse({"error": "llm_id ê°’ì´ ì—†ìŠµë‹ˆë‹¤."}, status=400)

    try:
        llm = LLM.objects.get(id=int(llm_id))
    except (LLM.DoesNotExist, ValueError):
        return JsonResponse({"error": "í•´ë‹¹ LLMì´ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜, llm_idê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤."}, status=404)

    if request.method == "POST" and request.FILES.get('image'):
        image_file = request.FILES['image']
        llm.llm_image.save(image_file.name, image_file)
        llm.save()
        # ì—…ë¡œë“œ í›„, í•„ìš”í•˜ë©´ ë‹¤ë¥¸ í˜ì´ì§€ë¡œ ì´ë™í•˜ê±°ë‚˜ ì´ë¯¸ì§€ ì—…ë¡œë“œ í˜ì´ì§€ ì¬ë Œë”
        return redirect(f"/customer_ai/upload_image/?llm_id={llm_id}")

    # GET ìš”ì²­ ì‹œ ì—…ë¡œë“œ í¼ ë Œë”
    return render(request, "customer_ai/upload_image.html", {"llm_id": llm_id, "llm": llm})


@csrf_exempt
@login_required
def generate_ai_image(request, llm_id):
    if request.method == "POST":
        prompt = request.POST.get("prompt", "")
        if not prompt:
            return JsonResponse({"error": "ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."}, status=400)

        try:
            # OpenAI ì´ë¯¸ì§€ ìƒì„± API í˜¸ì¶œ (DALLÂ·E ë“±)
            response = openai_client.images.generate(
                model="dall-e-3",  # ë˜ëŠ” ì ì ˆí•œ ëª¨ë¸ëª…
                prompt=prompt,
                n=1,
                size="512x512"
            )

            image_url = response.data[0].url

            # ì´ë¯¸ì§€ URLë¡œë¶€í„° ì´ë¯¸ì§€ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
            import requests
            img_data = requests.get(image_url).content

            # media/uploads/llm_images/ ê²½ë¡œì— ì €ì¥
            save_dir = os.path.join(settings.MEDIA_ROOT, "uploads/llm_images")
            os.makedirs(save_dir, exist_ok=True)
            filename = f"{uuid4().hex}.png"
            file_path = os.path.join(save_dir, filename)
            with open(file_path, "wb") as f:
                f.write(img_data)

            # LLM ê°ì²´ì— ì´ë¯¸ì§€ ê²½ë¡œ ì €ì¥
            llm = LLM.objects.get(id=llm_id)
            llm.llm_image = f"uploads/llm_images/{filename}"
            llm.save()

            return JsonResponse({"image_url": llm.llm_image.url})
        except Exception as e:
            return JsonResponse({"error": str(e)}, status=500)

    return JsonResponse({"error": "POST ìš”ì²­ë§Œ í—ˆìš©ë©ë‹ˆë‹¤."}, status=405)


openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

ALLOWED_IMAGE_EXTENSIONS = ['jpg', 'jpeg', 'png', 'webp', 'gif']
MAX_IMAGE_SIZE_MB = 5
ALLOWED_MODELS = ['gpt-4-vision-preview']

def is_allowed_image_file(filename):
    return filename.split('.')[-1].lower() in ALLOWED_IMAGE_EXTENSIONS

@csrf_exempt
def vision_process(request):
    if request.method != 'POST':
        return JsonResponse({'error': 'Invalid request'}, status=400)

    if 'image' not in request.FILES:
        return JsonResponse({'error': 'No image file provided'}, status=400)

    image_file = request.FILES['image']

    if not is_allowed_image_file(image_file.name):
        return JsonResponse({'error': 'ì§€ì›ë˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹ì…ë‹ˆë‹¤.'}, status=400)

    if not image_file.content_type.startswith("image/"):
        return JsonResponse({'error': 'ìœ íš¨í•œ ì´ë¯¸ì§€ íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤.'}, status=400)

    if image_file.size > MAX_IMAGE_SIZE_MB * 1024 * 1024:
        return JsonResponse({'error': 'ì´ë¯¸ì§€ íŒŒì¼ì´ ë„ˆë¬´ í½ë‹ˆë‹¤. (ìµœëŒ€ 5MB)'}, status=400)

    try:
        image_data = base64.b64encode(image_file.read()).decode("utf-8")
    except Exception as e:
        print("ğŸ”¥ Vision Read Error:")
        print(traceback.format_exc())
        return JsonResponse({'error': 'ì´ë¯¸ì§€ë¥¼ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ'}, status=500)

    custom_model = request.session.get('custom_model', 'gpt-4o-mini')
    if custom_model not in ALLOWED_MODELS:
        custom_model = 'gpt-4o-mini'

    custom_prompt = request.session.get('custom_prompt', 'ì´ ì´ë¯¸ì§€ì—ì„œ ë³´ì´ëŠ” ê²ƒì„ ì„¤ëª…í•´ ì£¼ì„¸ìš”.')

    try:
        response = openai_client.chat.completions.create(
            model=custom_model,
            messages=[
                {"role": "system", "content": "You are an assistant that describes images in detail."},
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": custom_prompt},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
                    ]
                }
            ]
        )
        ai_result = response.choices[0].message.content.strip()
        time.sleep(3)
        return JsonResponse({"result": ai_result})
    except Exception as e:
        print("ğŸ”¥ Vision OpenAI Error:")
        print(traceback.format_exc())
        return JsonResponse({"error": f"OpenAI Vision API error: {str(e)}"}, status=500)