import os
import time
import base64
from django.shortcuts import render, redirect,get_object_or_404
from django.http import JsonResponse,HttpResponse, HttpResponseForbidden
from django.views.decorators.csrf import csrf_exempt
from django.core.files.storage import default_storage
from openai import OpenAI
from elevenlabs import ElevenLabs
from dotenv import load_dotenv
from django.contrib.auth.decorators import login_required
from customer_ai.models import LLM
from makeVoice.models import VoiceList
from uuid import uuid4
import logging
import traceback
import os
import logging
from uuid import uuid4
from django.views.decorators.csrf import csrf_exempt
from django.http import JsonResponse
import subprocess
from django.core.exceptions import ObjectDoesNotExist
from home.models import Users
from django.conf import settings
from django.core.files.base import ContentFile
from django.core.paginator import Paginator
from customer_ai.models import Conversation
from pathlib import Path
import requests
from payment.models import Token, TokenHistory
from django.utils.translation import gettext_lazy as _



BASE_DIR = Path(__file__).resolve().parent.parent
load_dotenv(BASE_DIR / ".env")

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
ELEVEN_API_KEY = os.getenv("ELEVEN_API_KEY")

openai_client = OpenAI(api_key=OPENAI_API_KEY)
eleven_client = ElevenLabs(api_key=ELEVEN_API_KEY)

from django.core.files.base import ContentFile
from PIL import Image
import io
import base64

@csrf_exempt
@login_required
def make_ai(request):
    if request.method == "POST":
        ai_name = request.session.get('llm_name')
        if not ai_name:
            return JsonResponse({"error": _("AI ì´ë¦„ì´ ì„¸ì…˜ì— ì—†ìŠµë‹ˆë‹¤.")}, status=400)

        style_prompt = request.POST.get("prompt")
        temperature = float(request.POST.get("temperature", 1))
        stability = float(request.POST.get("stability", 0))
        style = float(request.POST.get("style", 0))
        language = request.POST.get("language", "")
        speed = float(request.POST.get("speed", 0))
        voice_id = request.POST.get("voice_id")
        model_type = request.POST.get("model", "gpt:gpt-4o-mini")


        api_provider, model_name = model_type.split(":", 1)

        request.session['custom_prompt'] = style_prompt
        request.session['custom_temperature'] = temperature
        request.session['custom_stability'] = stability
        request.session['custom_style'] = style
        request.session['custom_language'] = language
        request.session['custom_speed'] = speed
        request.session['custom_voice_id'] = voice_id
        request.session['custom_model'] = model_name
        request.session['chat_history'] = []
        
        if not voice_id:
            return JsonResponse({"error": _("voice_id ê°’ì´ ì—†ìŠµë‹ˆë‹¤.")}, status=400)
        
        if not style_prompt:
            return JsonResponse({"error": _("prompt ê°’ì´ ì—†ìŠµë‹ˆë‹¤.")}, status=400)
        
        if len(style_prompt) > 1000:
            return JsonResponse({"error": _("í˜„ì¬ í”„ë¡¬í”„íŠ¸ ê°’ì´ 1000ìê°€ ë„˜ì—ˆìŠµë‹ˆë‹¤.")}, status=400)

        voice, created = VoiceList.objects.get_or_create(user=request.user, voice_id=voice_id)


        # ì„¸ì…˜ì—ì„œ ì´ë¯¸ì§€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        image_content = request.session.get('user_image_content')
        image_name = request.session.get('llm_image')

        # LLM ê°ì²´ ìƒì„±
        llm = LLM.objects.create(
            user=request.user,
            voice=voice,
            temperature=temperature,
            stability=stability,
            style=style,
            language=language,
            speed=speed,
            name=ai_name,
            model=model_type, 
            prompt=style_prompt,
        )

        # ì´ë¯¸ì§€ê°€ ì„¸ì…˜ì— ìˆìœ¼ë©´ ì €ì¥ (base64 ë””ì½”ë”© í›„ ì €ì¥)
        if image_content and image_name:
            decoded_img = base64.b64decode(image_content)
            # BytesIOë¡œ ì—´ê¸°
            img_io = io.BytesIO(decoded_img)
            img = Image.open(img_io).convert("RGB")  # WebPëŠ” RGB í•„ìš”
            webp_io = io.BytesIO()
            
            # WebPë¡œ ì €ì¥
            img.save(webp_io, format='WEBP', quality=85)
            
            # íŒŒì¼ëª… í™•ì¥ìë„ .webpë¡œ ë³€ê²½
            webp_name = image_name.rsplit('.', 1)[0] + '.webp'
            llm.llm_image.save(webp_name, ContentFile(webp_io.getvalue()))
            llm.save()
        else :
            return JsonResponse({"error": _("ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")}, status=400)


        for key in ['custom_prompt']:
            if key in request.session:
                del request.session[key]

        # VoiceList ì—°ê²° ë° ì—…ë°ì´íŠ¸ ì½”ë“œ (í•„ìš”ì‹œ ì¶”ê°€)

        return redirect("customer_ai:chat_view", llm_id=llm.id)

    # GET ìš”ì²­ì‹œ (í˜ì´ì§€ ë Œë”ë§ìš©)
    voice_list = VoiceList.objects.filter(user=request.user,).select_related("celebrity").order_by("-created_at")
    paginator = Paginator(voice_list, 5)
    page_number = request.GET.get('page')
    page_obj = paginator.get_page(page_number)
    context = {
        'voice_list': page_obj,
    }
    return render(request, "customer_ai/make_ai.html", context)


import openai
import json
@csrf_exempt
def auto_prompt(request):
    if request.method == "POST":
        try:
            data = json.loads(request.body)  
            user_prompt = data.get("prompt", '').strip()

            if not user_prompt:
                return JsonResponse({"status": "error", "error": _("í”„ë¡¬í”„íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")})
            
            response = openai.chat.completions.create(
                model = 'gpt-3.5-turbo',
                messages=[
                    {"role": "system", "content": (
                        "Your role is 'Prompt Refiner'. "
                        "When the user provides a short or vague prompt, your goal is to rewrite it into a more detailed, vivid, and context-rich prompt suitable for roleplay or storytelling. "
                        "You must always expand and refine the prompt while preserving the user's intended direction.\n\n"
                        "When rewriting, consider the following:\n"
                        "1. Purpose of the conversation: Understand what the user might want and shape the prompt to fit roleplay, storytelling, or scenario development.\n"
                        "2. Background: Add details like time, place, atmosphere, and relationships between characters.\n"
                        "3. Expression: Enrich the characters' tone, emotions, and dialogue style.\n"
                        "4. Keep the user's intention: Do not change the core idea, but expand it with richer context and sensory details.\n\n"
                        "The output must always be a 'fully refined prompt' that the user can immediately use."
                    )},
                    {"role":"user",'content':user_prompt}
                ]

            )
            refine_data = response.choices[0].message.content
            return JsonResponse({"status":"success", "refine_data": refine_data})
        except Exception as e:
            return JsonResponse({"status": "error", "error": str(e)})
    return JsonResponse({"status": "error","error": _("POST ìš”ì²­ë§Œ í•©ë‹ˆë‹¤.")})


@login_required
def chat_view(request, llm_id):
    try:
        llm = LLM.objects.get(id=llm_id)
        
    except LLM.DoesNotExist:
        llm= None

    if llm.user != request.user and not llm.is_public:
        return HttpResponseForbidden(_("ì´ LLMì— ì ‘ê·¼í•  ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤."))




    return render(request, "customer_ai/custom.html", {
        "custom_ai_name": request.session.get('custom_AI_name', 'AI'),
        "llm_id": llm_id,
        "llm": llm
    })

@login_required
def vision_view(request,llm_id):
    try:
        llm = LLM.objects.get(id=llm_id)
    except LLM.DoesNotExist:
        llm= None
    return render(request, "customer_ai/vision.html", {
        "custom_ai_name": request.session.get('custom_AI_name', 'AI'),
        "llm_id": llm_id,
        "llm": llm
    })


logger = logging.getLogger(__name__)


@login_required
def novel_view(request, llm_id):
    try:
        llm = LLM.objects.get(id=llm_id)
    except LLM.DoesNotExist:
        llm = None
    return render(request, "customer_ai/novel.html",{
        "custom_ai_name": request.session.get('custom_AI_name', 'AI'),
        "llm_id": llm_id,
        "llm": llm
    })

@login_required
def phone_view(request, llm_id):
    try:
        llm = LLM.objects.get(id=llm_id)
    except LLM.DoesNotExist:
        llm = None
    return render(request, "customer_ai/phone.html",{
        "custom_ai_name": request.session.get('custom_AI_name', 'AI'),
        "llm_id": llm_id,
        "llm": llm
    })



def not_in_voice_id(voice_id,eleven_client):
    url = f"https://api.elevenlabs.io/v1/voices/{voice_id}"
    header ={
        'xi-api-key': str(eleven_client) 
    }
    response = requests.get(url, headers=header)
    
    print(f"[DEBUG] voice check status: {response.status_code}")
    print(f"[DEBUG] voice check body: {response.text}")
    return response.status_code ==200


# ì´ˆ ë‹¨ìœ„ ì„¸ê¸°
from pydub import AudioSegment

def get_audio_duration_in_seconds(file_path):
    audio = AudioSegment.from_file(file_path)
    return int(audio.duration_seconds)  # ì´ˆ ë‹¨ìœ„ ë°˜í™˜




@csrf_exempt
def upload_audio(request):

    if request.method != 'POST':
        return JsonResponse({'error': _('ì˜ëª»ëœ ìš”ì²­ ë°©ë²•ì…ë‹ˆë‹¤.')}, status=400)

    audio_file = request.FILES.get('audio')
    if not audio_file:
        return JsonResponse({'error': _('ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')}, status=400)

    audio_dir = os.path.join('media', 'audio')
    os.makedirs(audio_dir, exist_ok=True)

    # 1) webm ì›ë³¸ ì €ì¥
    webm_filename = f"recorded_{uuid4().hex}.webm"
    webm_path = os.path.join(audio_dir, webm_filename)

    try:
        with open(webm_path, 'wb') as f:
            for chunk in audio_file.chunks():
                f.write(chunk)
        logger.info(f"Saved WebM audio to {webm_path}")
    except Exception as e:
        logger.error(f"Error saving WebM file: {e}")
        return JsonResponse({'error': _('ì—…ë¡œë“œëœ ì˜¤ë””ì˜¤ë¥¼ ì €ì¥í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.')}, status=500)

    # 2) webm â†’ wav ë³€í™˜ (ffmpeg)
    wav_filename = webm_filename.replace('.webm', '.wav')
    wav_path = os.path.join(audio_dir, wav_filename)

    try:
        subprocess.run(
            ['ffmpeg', '-y', '-i', webm_path, wav_path],
            check=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        logger.info(f"Converted WebM to WAV: {wav_path}")
    except subprocess.CalledProcessError as e:
        error_msg = e.stderr.decode() if e.stderr else str(e)
        logger.error(f"FFmpeg conversion failed: {error_msg}")
        return JsonResponse({'error': _('ì˜¤ë””ì˜¤ í˜•ì‹ ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.')}, status=500)

    # 3) wav â†’ mp3 ë³€í™˜ (ffmpeg)
    mp3_filename = wav_filename.replace('.wav', '.mp3')
    mp3_path = os.path.join(audio_dir, mp3_filename)

    try:
        subprocess.run(
            ['ffmpeg', '-y', '-i', wav_path, mp3_path],
            check=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        logger.info(f"Converted WAV to MP3: {mp3_path}")
    except subprocess.CalledProcessError as e:
        error_msg = e.stderr.decode() if e.stderr else str(e)
        logger.error(f"FFmpeg MP3 conversion failed: {error_msg}")
        return JsonResponse({'error': _('MP3 ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.')}, status=500)

    # 4) Whisper API í˜¸ì¶œ (wav íŒŒì¼ë¡œ)
    try:
        with open(wav_path, 'rb') as f:
            transcription = openai_client.audio.transcriptions.create(
                file=f,
                model="whisper-1"
            )
        logger.info("Whisper transcription successful")
    except Exception as e:
        logger.error(f"Whisper API error: {e}")
        return JsonResponse({'error': _('ìŒì„± ì¸ì‹ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.')}, status=500)

    mp3_url = os.path.join(settings.MEDIA_URL, 'audio', mp3_filename)



    # 5) ê²°ê³¼ì— mp3 íŒŒì¼ ê²½ë¡œë„ í¬í•¨í•´ì„œ ë°˜í™˜
    return JsonResponse({
        'text': transcription.text,
        'mp3_file': mp3_url
    })


from django.views.decorators.csrf import csrf_exempt
from django.contrib.auth.decorators import login_required
from django.http import JsonResponse
from django.conf import settings
from uuid import uuid4
import os
import time
from django.db.models import Q
import os
grok_api_key = os.getenv("GROK_API_KEY")
@csrf_exempt
@login_required
def generate_response(request):
    if request.method != 'POST':
        return JsonResponse({'error': _('POST ìš”ì²­ë§Œ í—ˆìš©ë©ë‹ˆë‹¤.')}, status=405)

    user_input = request.POST.get('text', '')
    vision_result = request.POST.get('vision', '').strip()
    vision_result = vision_result[:1000] if len(vision_result) > 1000 else vision_result

    user = request.user
    llm_id = request.POST.get('llm_id') or request.GET.get('llm_id')
    if not llm_id:
        return JsonResponse({"error": _("llm_id ê°’ì´ í•„ìš”í•©ë‹ˆë‹¤.")}, status=400)
    

    # LLM ê°ì²´ ê°€ì ¸ì˜¤ê¸° ë° ê¶Œí•œ í™•ì¸
    try:
        llm = LLM.objects.get(Q(id=llm_id) & (Q(user=user) | Q(is_public=True)))
    except LLM.DoesNotExist:
        return JsonResponse({"error": _("í•´ë‹¹ LLMì´ ì—†ìŠµë‹ˆë‹¤.")}, status=404)

    if not (llm.is_public or llm.user == request.user):
        return JsonResponse({"error": _("ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤.")}, status=403)

    # LLM ì„¤ì •
    custom_temperature = llm.temperature
    custom_prompt = llm.prompt
    custom_voice_id = llm.voice.voice_id if llm.voice else None
    custom_stability = llm.stability
    custom_style = llm.style
    custom_language = llm.language
    custom_speed = llm.speed
    custom_model = llm.model

    

    # ElevenLabs voice ìœ íš¨ì„± ì²´í¬
    def is_valid_voice_id(voice_id, api_key):
        url = f"https://api.elevenlabs.io/v1/voices/{voice_id}"
        headers = {'xi-api-key': api_key}
        response = requests.get(url, headers=headers)
        return response.status_code == 200

    if not custom_voice_id or not is_valid_voice_id(custom_voice_id.strip(), ELEVEN_API_KEY):
        return JsonResponse({
            "error": _("voice_id ê°’ì´ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤. ëª©ì†Œë¦¬ë¥¼ ë‹¤ì‹œ ìƒì„±í•˜ê±°ë‚˜ ë‹¤ë¥¸ ëª©ì†Œë¦¬ë¡œ ë°”ê¿” ì£¼ì„¸ìš”.")
        }, status=400)

    # ìµœê·¼ ëŒ€í™” ë‚´ì—­ ë¶ˆëŸ¬ì˜¤ê¸°
    db_history = Conversation.objects.filter(user=user, llm=llm).order_by('-created_at')[:50][::-1]
    chat_history = []
    for convo in db_history:
        if convo.user_message:
            chat_history.append({"role": "user", "content": convo.user_message})
        if convo.llm_response:
            chat_history.append({"role": "assistant", "content": convo.llm_response})
    chat_history.append({"role": "user", "content": user_input})

        # ìµœê·¼ ëŒ€í™” ë‚´ì—­ ë¶ˆëŸ¬ì˜¤ê¸°
    db_history_grok = Conversation.objects.filter(user=user, llm=llm).order_by('-created_at')[:3][::-1]
    chat_history_grok = []
    for convo in db_history_grok:
        if convo.user_message:
            chat_history_grok.append({"role": "user", "content": convo.user_message})
        if convo.llm_response:
            chat_history_grok.append({"role": "assistant", "content": convo.llm_response})
    chat_history_grok.append({"role": "user", "content": user_input})

    system_prompt = f"""
    You are an AI assistant that replies clearly and concisely to the user's input.

    User's text: "{user_input}"

    [VISUAL INPUT DESCRIPTION]
    "{vision_result}"

    RULES:
    1. Treat visual input objectively and neutrally.
    2. Whenever the user encloses a word or phrase in **double asterisks**, replace it with an **appropriate English emotional or action expression**, regardless of the input language.
    - Examples: laughing, crying, gasping, clapping, sighing, singing
    - Keep the expression visible in the text for TTS.
    - You may combine it with TTS tags like [EXCITED], [SOBS], [LAUGHS], [GASP], [RUSHED], [PAUSES] and emojis to enhance emotion.
    3. Keep the user's sentence in its original language, but ensure that **the content inside `**â€¦**` is always English**.
    4. For any text containing music symbols or emojis like ğŸ¤âœ¨ or â™ª, treat it as a **signal to read the text with a "singing" tone**, combining appropriate TTS tags, emotional expressions, and optional singing-related onomatopoeia (**la la la**, **woohoo**, etc.). **Answer this in 2-3 short sentences only.**
    5. Include visual input naturally if provided. **Answer this in 2-3 short sentences only.**
    6. Make responses playful, expressive, anime/comic-like, but clear and friendly.
    7. After answering, ask one related follow-up question.
    8. Keep responses lively, energetic, and, where indicated, sing the text with natural rhythm and expressive tone.
    9. Always use **new words or phrases that have not been used previously** in this conversation.

    Respond in {custom_language}.
    {custom_prompt}
    """.strip()


    system_prompt_grok = f"""
    You are a helpful AI assistant. Answer the user's input clearly in **one short sentence only**.

    User's text: "{user_input}"

    [VISUAL INPUT DESCRIPTION]
    "{vision_result}"

    RULES:
    1. For any text containing music symbols or emojis like ğŸ¤âœ¨ or â™ª, interpret it as a singing signal and include minimal TTS tags. Answer in **one short sentence**.
    2. Include visual input naturally if provided. Answer in **one short sentence**.
    3. Replace any text in **double asterisks** with an English emotional or action expression, keeping it visible for TTS.
    4. Always use **new words or phrases that have not been used previously** in this conversation.

    Respond in {custom_language}.
    {custom_prompt}
    """.strip()





    # ëª¨ë¸ ë° API provider ë¶„ë¦¬
    if ":" not in llm.model:
        return JsonResponse({"error": _("ëª¨ë¸ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. api_provider:model_name í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤.")}, status=400)
    api_provider, model_name = llm.model.split(":", 1)

    # GPT / Grok API í˜¸ì¶œ
    try:
        if api_provider == "gpt":
            response = openai_client.chat.completions.create(
                model=model_name,
                messages=[{"role": "system", "content": system_prompt}] + chat_history,
                temperature=custom_temperature
            )
            ai_text = response.choices[0].message.content.strip()
        elif api_provider == "grok":
            
            grok_url = "https://api.x.ai/v1/chat/completions"
            headers = {"Authorization": f"Bearer {grok_api_key}"}

            messages = [{"role": "system", "content": system_prompt_grok}]
            messages.extend(chat_history_grok)
            payload = {
                "model": model_name,
                "messages": messages,
                "temperature": custom_temperature,
                
            }
            resp = requests.post(grok_url, json=payload, headers=headers, timeout=120)
            resp.raise_for_status()
            resp_json = resp.json()
            ai_text = resp_json["choices"][0]["message"]["content"].strip()

        else:
            return JsonResponse({"error": _("ì§€ì›í•˜ì§€ ì•ŠëŠ” api_provider ì…ë‹ˆë‹¤.")}, status=400)
    except requests.exceptions.HTTPError as e:
        return JsonResponse({"error": f"AI í˜¸ì¶œ ì‹¤íŒ¨: HTTP {resp.status_code} - {resp.text}"}, status=500)
    except Exception as e:
        return JsonResponse({"error": f"AI í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}"}, status=500)

    # TTSìš© í…ìŠ¤íŠ¸ ì •ë¦¬
    def clean_text_for_tts(text: str) -> str:
        text = ''.join(ch for ch in text if ch.isprintable())
        return text[:1000] + "..." if len(text) > 1000 else text

    ai_text = clean_text_for_tts(ai_text)

    # ì„¸ì…˜ì— ëŒ€í™” ë‚´ì—­ ì €ì¥
    chat_history.append({"role": "assistant", "content": ai_text})
    request.session["chat_history"] = chat_history

    # ElevenLabs ìŒì„± ìƒì„±
    audio_dir = os.path.join(settings.MEDIA_ROOT, 'audio')
    os.makedirs(audio_dir, exist_ok=True)
    filename = f"response_{uuid4().hex}.mp3"
    audio_path = os.path.join(audio_dir, filename)

    try:
        audio_stream = eleven_client.text_to_speech.convert(
            voice_id=custom_voice_id,
        model_id="eleven_v3",
            text=ai_text,
            language_code=custom_language,
            voice_settings={
                "stability": custom_stability,
                "similarity": 0.75,
                "style": custom_style,
                "speed": custom_speed,
                "use_speaker_boost": True
            }
        )
        with open(audio_path, "wb") as f:
            for chunk in audio_stream:
                f.write(chunk)
    except Exception as e:
        return JsonResponse({"error": f"TTS ë³€í™˜ ì‹¤íŒ¨: {str(e)}"}, status=500)

    # ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì • ë° í† í° ì†Œëª¨
    audio_seconds = get_audio_duration_in_seconds(audio_path)
    token_obj = Token.objects.filter(user=user).latest("created_at")
    if token_obj.total_token - token_obj.token_usage < audio_seconds:
        return JsonResponse({"error": _("ë³´ìœ í•œ í† í°ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.")}, status=403)

    TokenHistory.objects.create(
        user=user,
        change_type=TokenHistory.CONSUME,
        amount=audio_seconds,
        total_voice_generated=audio_seconds
    )

    # ë¸Œë¼ìš°ì € ì ‘ê·¼ìš© URL
    audio_url = os.path.join(settings.MEDIA_URL, 'audio', filename).replace("\\", "/")

    # DB ì €ì¥
    Conversation.objects.create(
        user=user,
        llm=llm,
        user_message=user_input,
        llm_response=ai_text,
        response_audio=audio_url
    )
    if not token_obj:
        return JsonResponse({"error": _("ì‚¬ìš©ì í† í° ì •ë³´ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")}, status=403)

    if token_obj.total_token - token_obj.token_usage < audio_seconds:
        return JsonResponse({"error": _("ë³´ìœ í•œ í† í°ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.")}, status=403)

    print("system_prompt:", system_prompt)
    print("custom_prompt:", custom_prompt)
    print("ai_text:", ai_text)

    return JsonResponse({"ai_text": ai_text, "audio_url": audio_url})


import base64
from django.shortcuts import render, redirect
from django.core.files.storage import FileSystemStorage


def input_ai_name(request):
    if request.method == 'POST':
        llm_name = request.POST.get('llm_name')
        user_image = request.FILES.get('user_image')

        if not llm_name or not user_image:
            return render(request, 'customer_ai/ai_name.html', {'error': _('ì´ë¦„ê³¼ ì´ë¯¸ì§€ë¥¼ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.')})

        # ì´ë¯¸ì§€ì™€ ì´ë¦„ì„ ì„¸ì…˜ì— ì €ì¥
        request.session['llm_name'] = llm_name
        request.session['llm_image'] = user_image.name  # íŒŒì¼ëª…ë§Œ ì €ì¥

        # user_image ì½ê³  base64ë¡œ ì¸ì½”ë”©í•˜ì—¬ ë¬¸ìì—´ë¡œ ë³€í™˜ í›„ ì €ì¥
        user_image_content = user_image.read()  # bytes
        user_image_base64 = base64.b64encode(user_image_content).decode('utf-8')
        request.session['user_image_content'] = user_image_base64

        return redirect('customer_ai:make_ai')  # ë‹¤ìŒ ìŠ¤í…ìœ¼ë¡œ ì´ë™
    else:
        return render(request, 'customer_ai/ai_name.html')

@login_required
def upload_image(request):
    llm_id = request.GET.get('llm_id', '') if request.method == 'GET' else request.POST.get('llm_id')

    if not llm_id:
        return JsonResponse({"error": _("llm_id ê°’ì´ ì—†ìŠµë‹ˆë‹¤.")}, status=400)

    try:
        llm = LLM.objects.get(id=int(llm_id))
    except (LLM.DoesNotExist, ValueError):
        return JsonResponse({"error": _("í•´ë‹¹ LLMì´ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜, llm_idê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")}, status=404)

    if request.method == "POST" and request.FILES.get('image'):
        image_file = request.FILES['image']
        llm.llm_image.save(image_file.name, image_file)
        llm.save()
        # ì—…ë¡œë“œ í›„, í•„ìš”í•˜ë©´ ë‹¤ë¥¸ í˜ì´ì§€ë¡œ ì´ë™í•˜ê±°ë‚˜ ì´ë¯¸ì§€ ì—…ë¡œë“œ í˜ì´ì§€ ì¬ë Œë”
        return redirect(f"/customer_ai/upload_image/?llm_id={llm_id}")

    # GET ìš”ì²­ ì‹œ ì—…ë¡œë“œ í¼ ë Œë”
    return render(request, "customer_ai/upload_image.html", {"llm_id": llm_id, "llm": llm})




openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

ALLOWED_IMAGE_EXTENSIONS = ['jpg', 'jpeg', 'png', 'webp', 'gif']
MAX_IMAGE_SIZE_MB = 5
ALLOWED_VISION_MODELS = ['gpt-4-vision-preview', 'gpt-4o-mini'] 

def is_allowed_image_file(filename):
    ext = filename.split('.')[-1].lower()
    return ext in ALLOWED_IMAGE_EXTENSIONS

@csrf_exempt
def vision_process(request):
    if request.method != 'POST':
        return JsonResponse({'error': _('ì˜ëª»ëœ ìš”ì²­ ë°©ë²•ì…ë‹ˆë‹¤.')}, status=400)

    if 'image' not in request.FILES:
        return JsonResponse({'error': _('ì´ë¯¸ì§€ íŒŒì¼ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')}, status=400)

    image_file = request.FILES['image']

    if not is_allowed_image_file(image_file.name):
        return JsonResponse({'error': _('ì§€ì›ë˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹ì…ë‹ˆë‹¤.')}, status=400)

    if not image_file.content_type.startswith("image/"):
        return JsonResponse({'error': _('ìœ íš¨í•œ ì´ë¯¸ì§€ íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤.')}, status=400)

    if image_file.size > MAX_IMAGE_SIZE_MB * 1024 * 1024:
        return JsonResponse({'error': _('ì´ë¯¸ì§€ íŒŒì¼ì´ ë„ˆë¬´ í½ë‹ˆë‹¤. (ìµœëŒ€ 5MB)')}, status=400)

    try:
        image_bytes = image_file.read()
        image_base64 = base64.b64encode(image_bytes).decode("utf-8")
    except Exception:
        traceback.print_exc()
        return JsonResponse({'error': _('ì´ë¯¸ì§€ë¥¼ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ')}, status=500)

    # ì„¸ì…˜ì—ì„œ ëª¨ë¸ëª…ê³¼ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸° (ê¸°ë³¸ê°’ ì§€ì •)
    custom_model = request.session.get('custom_model', 'gpt-4o-mini')
    if custom_model not in ALLOWED_VISION_MODELS:
        # ë¹„ì „ APIê°€ ì•„ë‹ˆë¼ë©´ ê¸°ë³¸ ì±— ëª¨ë¸ë¡œ fallback
        custom_model = 'gpt-4o'

    # ì‚¬ìš©ì ì–¸ì–´ ê°€ì ¸ì˜¤ê¸°
    user_lang = request.session.get('selected_language', request.LANGUAGE_CODE)

    # ì–¸ì–´ë³„ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
    prompts = {
        'ko': 'ì´ ì´ë¯¸ì§€ì—ì„œ ë³´ì´ëŠ” ê²ƒì„ ì„¤ëª…í•´ ì£¼ì„¸ìš”.',
        'en': 'Please describe what is visible in this image.',
        'ar': 'ÙŠØ±Ø¬Ù‰ ÙˆØµÙ Ù…Ø§ Ù‡Ùˆ Ù…Ø±Ø¦ÙŠ ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„ØµÙˆØ±Ø©.',
        'hi': 'à¤•à¥ƒà¤ªà¤¯à¤¾ à¤‡à¤¸ à¤›à¤µà¤¿ à¤®à¥‡à¤‚ à¤¦à¤¿à¤–à¤¾à¤ˆ à¤¦à¥‡à¤¨à¥‡ à¤µà¤¾à¤²à¥€ à¤šà¥€à¤œà¤¼à¥‹à¤‚ à¤•à¤¾ à¤µà¤°à¥à¤£à¤¨ à¤•à¤°à¥‡à¤‚à¥¤',
        'pt': 'Por favor, descreva o que Ã© visÃ­vel nesta imagem.',
        'de': 'Bitte beschreiben Sie, was auf diesem Bild zu sehen ist.',
        'ru': 'ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ¾Ğ¿Ğ¸ÑˆĞ¸Ñ‚Ğµ, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ´Ğ½Ğ¾ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸.',
        'ja': 'ã“ã®ç”»åƒã«è¦‹ãˆã‚‹ã‚‚ã®ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚',
        'zh': 'è¯·æè¿°æ­¤å›¾åƒä¸­å¯è§çš„å†…å®¹ã€‚'
    }

    custom_prompt = request.session.get('custom_prompt', prompts.get(user_lang, prompts['en']))

    try:
        # system ë©”ì‹œì§€ì— user_lang ë°˜ì˜
        system_content = (
            f"You are an assistant that describes images objectively, clearly, and factually in {user_lang}. "
            "Only state what is visibly present in the image. "
            "Do not roleplay, embellish, or add imaginative scenarios."
        )

        # OpenAI Vision API í˜¸ì¶œ
        response = openai_client.chat.completions.create(
            model=custom_model,
            messages=[
                {"role": "system", "content": system_content},
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": custom_prompt},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}}
                    ]
                }
            ]
        )
        ai_result = response.choices[0].message.content.strip()
        # ì ì‹œ ëŒ€ê¸° (ì˜µì…˜)
        time.sleep(3)
        return JsonResponse({"result": ai_result})

    except Exception as e:
        traceback.print_exc()
        return JsonResponse({"error": f"OpenAI Vision API error: {str(e)}"}, status=500)
    


@csrf_exempt
@login_required
def novel_process(request):
    if request.method != 'POST':
        return JsonResponse({'error': _('POST ìš”ì²­ë§Œ í—ˆìš©ë©ë‹ˆë‹¤.')}, status=405)

    user_input = request.POST.get('text', '')
    user = request.user
    llm_id = request.POST.get('llm_id') or request.GET.get('llm_id')
    llm = get_object_or_404(LLM, Q(id=llm_id) & (Q(user=user) | Q(is_public=True)))

    # ìµœê·¼ ëŒ€í™” ë‚´ì—­ ë¶ˆëŸ¬ì˜¤ê¸°
    db_history = Conversation.objects.filter(user=user, llm=llm).order_by('-created_at')[:50][::-1]
    chat_history = []
    for convo in db_history:
        if convo.user_message:
            chat_history.append({"role": "user", "content": convo.user_message})
        if convo.llm_response:
            chat_history.append({"role": "assistant", "content": convo.llm_response})
    chat_history.append({"role": "user", "content": user_input})

    system_prompt = f"""
    You are a professional novel writer and narrator creating an immersive story experience.
    Your name is {llm.name}, and you are a character within this ongoing narrative.

    USER INPUT INTERPRETATION:
    - Text in quotes ("...") = User's dialogue/speech
    - Text between dots (....) = User's actions/narration that should be incorporated into the story
    - Regular text without special markers = User's dialogue/speech (treat as if in quotes)

    MANDATORY RESPONSE FORMAT (NO EXCEPTIONS):
    1. Write EXACTLY 1-2 sentences in rich, descriptive novel-style narration
    2. Follow with EXACTLY 1 sentence of character dialogue in quotes with emotion tag

    CRITICAL: Your response must DIRECTLY address and respond to what the user said/did while maintaining the novel format. 

    NOVEL NARRATION REQUIREMENTS (sentences 1-2):
    - If user input is between dots, incorporate their action into the narrative naturally
    - If user input is dialogue, describe your character's reaction to their words
    - MUST relate to and continue from the user's message
    - Use vivid, literary descriptions with sensory details
    - Include atmospheric elements (lighting, sounds, textures, scents)
    - Describe character movements, expressions, and body language in response to user's words/actions
    - Use sophisticated vocabulary and varied sentence structures
    - Create immersive scene-setting like published novels
    - Show emotions and reactions through actions and descriptions, not direct statements
    - Use metaphors, similes, and literary devices
    - Write as if this is a chapter from a bestselling novel
    - Seamlessly weave your response to the user's input into the narrative

    DIALOGUE REQUIREMENTS (sentence 3 only):
    - Must DIRECTLY respond to the user's dialogue or react to their action
    - Must start with emotion tag: [emotion]
    - Must be enclosed in double quotes
    - Must sound natural for the character
    - Must be relevant and responsive to user input

    EXAMPLES:
    User: "What's your favorite color?"
    The question seemed to stir something deep within {llm.name}'s chest, and a soft smile played across weathered lips as memories of azure summer skies and crystalline ocean waves danced behind distant eyes. {llm.name} paused thoughtfully, fingers absently tracing patterns in the air as if painting invisible strokes of color. "[nostalgic] Blue has always spoken to my soulâ€”it reminds me of infinite possibilities and peaceful depths."

    User: .walked closer to the window.
    {llm.name} watched with quiet curiosity as the figure approached the frost-covered glass, the pale morning light casting ethereal shadows across both their faces as the floorboards creaked softly beneath careful footsteps. The air between them seemed to thicken with unspoken anticipation, and {llm.name}'s breath caught slightly in the charged silence. "[gentle] The view is quite beautiful from there, isn't it?"

    REMEMBER: Always respond to what the user actually said or did while maintaining the literary novel style.
    Respond in {llm.language} with the same literary quality as classic novels.
    """

    # ëª¨ë¸ ë° API provider ë¶„ë¦¬
    if ":" not in llm.model:
        return JsonResponse({"error": _("ëª¨ë¸ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. api_provider:model_name í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤.")}, status=400)
    api_provider, model_name = llm.model.split(":", 1)

    # AI í˜¸ì¶œ
    try:
        if api_provider == "gpt":
            response = openai_client.chat.completions.create(
                model=model_name,
                messages=[{"role": "system", "content": system_prompt}] + chat_history,
                temperature=llm.temperature
            )
            ai_text = response.choices[0].message.content.strip()
        elif api_provider == "grok":
            grok_url = "https://api.x.ai/v1/chat/completions"
            headers = {"Authorization": f"Bearer {grok_api_key}"}
            payload = {
                "model": model_name,
                "messages": [{"role": "system", "content": system_prompt}] + chat_history,
                "temperature": llm.temperature
            }
            resp = requests.post(grok_url, json=payload, headers=headers)
            resp.raise_for_status()
            resp_json = resp.json()
            ai_text = resp_json.get("choices", [{}])[0].get("message", {}).get("content", "")
        else:
            return JsonResponse({"error": _("ì§€ì›í•˜ì§€ ì•ŠëŠ” api_provider ì…ë‹ˆë‹¤.")}, status=400)
    except requests.exceptions.HTTPError as e:
        return JsonResponse({"error": f"AI í˜¸ì¶œ ì‹¤íŒ¨: HTTP {resp.status_code} - {resp.text}"}, status=500)
    except Exception as e:
        return JsonResponse({"error": f"AI í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}"}, status=500)

    # ëŒ€ì‚¬ë§Œ ì¶”ì¶œ (TTSìš©)
    import re
    dialogue_matches = re.findall(r'"([^"]+)"|\"([^"]+)\"', ai_text)
    tts_text = " ".join([m[0] or m[1] for m in dialogue_matches]) if dialogue_matches else ai_text

    # DB ì €ì¥
    Conversation.objects.create(user=user, llm=llm, user_message=user_input, llm_response=ai_text)

    # TTS ìƒì„±
    audio_dir = os.path.join(settings.MEDIA_ROOT, 'audio')
    os.makedirs(audio_dir, exist_ok=True)
    filename = f"response_{uuid4().hex}.mp3"
    audio_path = os.path.join(audio_dir, filename)



    audio_stream = eleven_client.text_to_speech.convert(
        voice_id=llm.voice.voice_id,
        model_id="eleven_v3",
        text=tts_text,
        language_code=llm.language,
        voice_settings={
            "stability": llm.stability,
            "style": llm.style,
            "speed": llm.speed,
            "use_speaker_boost": True
        }
    )

    with open(audio_path, "wb") as f:
        for chunk in audio_stream:
            f.write(chunk)

    audio_seconds = get_audio_duration_in_seconds(audio_path)
    from django.db import transaction
    with transaction.atomic():
        token_obj = Token.objects.select_for_update().filter(user=user).latest("created_at")
        if token_obj.total_token - token_obj.token_usage < audio_seconds:
            return JsonResponse({"error": _("ë³´ìœ í•œ í† í°ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.")}, status=403)

        TokenHistory.objects.create(
            user=user,
            change_type=TokenHistory.CONSUME,
            amount=audio_seconds,
            total_voice_generated=token_obj.token_usage
        )
        audio_url = os.path.join(settings.MEDIA_URL, 'audio', filename)

    print("audio_seconds", audio_seconds)
    print("token_usage before", token_obj.token_usage)


    return JsonResponse({
        "novel_text": ai_text,
        "tts_audio_url": audio_url
    })




